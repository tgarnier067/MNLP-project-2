{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc942b8237dc4f8eb1c3feaa742aa910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d15e6eedeb5b4916be130701142add3d",
              "IPY_MODEL_b289e01b8f86432e980506f560c337c6",
              "IPY_MODEL_d5fb4926805b4e01bf4a3542169df15f"
            ],
            "layout": "IPY_MODEL_ed6df9b6131f427fa8a013ec78b76091"
          }
        },
        "d15e6eedeb5b4916be130701142add3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f4ada18ad44603a50356f4030b87ab",
            "placeholder": "​",
            "style": "IPY_MODEL_03f388ae10e446bd9a1999b09a7b4b51",
            "value": "config.json: 100%"
          }
        },
        "b289e01b8f86432e980506f560c337c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2809a1ca55b94384adff6fd8e7391550",
            "max": 728,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a40d0a10af040549b8383b21d474b0c",
            "value": 728
          }
        },
        "d5fb4926805b4e01bf4a3542169df15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f74e02ea166445a3a6fa34f052a49f0a",
            "placeholder": "​",
            "style": "IPY_MODEL_241f389b67cf4c83938edd7e112b7a77",
            "value": " 728/728 [00:00&lt;00:00, 12.2kB/s]"
          }
        },
        "ed6df9b6131f427fa8a013ec78b76091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0f4ada18ad44603a50356f4030b87ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f388ae10e446bd9a1999b09a7b4b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2809a1ca55b94384adff6fd8e7391550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a40d0a10af040549b8383b21d474b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f74e02ea166445a3a6fa34f052a49f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241f389b67cf4c83938edd7e112b7a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08e88f65803844a0b615a8370c8dee1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcaef48f103c4b2da6aab082cba4ce79",
              "IPY_MODEL_aa89cdada5fd4baab5a317926b8e9735",
              "IPY_MODEL_51fffdf0888146359dc64afb6f68c1a4"
            ],
            "layout": "IPY_MODEL_2f512852a1cd45298570639f2858eb73"
          }
        },
        "fcaef48f103c4b2da6aab082cba4ce79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b731ae7fc2824af880e7d6f8e93f7948",
            "placeholder": "​",
            "style": "IPY_MODEL_23191f826e4942d595f5046922d80fd8",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "aa89cdada5fd4baab5a317926b8e9735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff57d1b9ff0a4ca9b1221245eca4776d",
            "max": 62311002,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf4e816de7224b84add1fb422f75770f",
            "value": 62311002
          }
        },
        "51fffdf0888146359dc64afb6f68c1a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d970982bb36415f94878b1ade043781",
            "placeholder": "​",
            "style": "IPY_MODEL_5c22fa74557948359bf175f14059dc2e",
            "value": " 62.3M/62.3M [00:02&lt;00:00, 41.3MB/s]"
          }
        },
        "2f512852a1cd45298570639f2858eb73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b731ae7fc2824af880e7d6f8e93f7948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23191f826e4942d595f5046922d80fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff57d1b9ff0a4ca9b1221245eca4776d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf4e816de7224b84add1fb422f75770f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d970982bb36415f94878b1ade043781": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c22fa74557948359bf175f14059dc2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de4a464ea1dd40abb99a8787819269c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c11b5478f80142b289a49e968658f8ed",
              "IPY_MODEL_c51cddd3b4314183a8aa781d838fa0ee",
              "IPY_MODEL_eee95196c3cd4306875547c6d3b1527a"
            ],
            "layout": "IPY_MODEL_e3bc914458b7443fa9e77fdc399c81bd"
          }
        },
        "c11b5478f80142b289a49e968658f8ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_669467b4e1414673bc32328ce5b986a2",
            "placeholder": "​",
            "style": "IPY_MODEL_7329a566985e40d6951acf0eb6cc5529",
            "value": "model.safetensors: 100%"
          }
        },
        "c51cddd3b4314183a8aa781d838fa0ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39b6df90c9794a4daf71122240f9db61",
            "max": 62293080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d510188131ef443cad2b34fd5697eb3f",
            "value": 62293080
          }
        },
        "eee95196c3cd4306875547c6d3b1527a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea93061fa70d43eda8ca7e338067d02f",
            "placeholder": "​",
            "style": "IPY_MODEL_f81d49eecd774465bb22aa971a99a705",
            "value": " 62.3M/62.3M [00:01&lt;00:00, 45.1MB/s]"
          }
        },
        "e3bc914458b7443fa9e77fdc399c81bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "669467b4e1414673bc32328ce5b986a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7329a566985e40d6951acf0eb6cc5529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39b6df90c9794a4daf71122240f9db61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d510188131ef443cad2b34fd5697eb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea93061fa70d43eda8ca7e338067d02f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81d49eecd774465bb22aa971a99a705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarnier067/MNLP-project-2/blob/main/02_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "ngG6cCRETRoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Vqml8T0sB5Dw",
        "outputId": "bcfffa24-9952-4478-ffa4-c312f5462da6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fb1143a-22be-4148-9a7e-c1f3e60181ee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1fb1143a-22be-4148-9a7e-c1f3e60181ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the_vampyre_clean.json to the_vampyre_clean.json\n",
            "Saving the_vampyre_ocr.json to the_vampyre_ocr.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"the_vampyre_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    clean_data = json.load(f)\n",
        "\n",
        "with open(\"the_vampyre_ocr.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ocr_data = json.load(f)"
      ],
      "metadata": {
        "id": "l2_61Rw7CzvQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "K3gEh-A-TdxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_values_dict(d):\n",
        "    \"\"\"\n",
        "    Concat values of a dict, seperating each element with '\\n'\n",
        "\n",
        "    Args:\n",
        "        d (dict): Dictionnary\n",
        "\n",
        "    Returns:\n",
        "        str: concatenated text\n",
        "    \"\"\"\n",
        "    return '\\n'.join(d.get(str(i), \"\") for i in range(48))\n",
        "\n",
        "clean_data_text = concat_values_dict(clean_data)"
      ],
      "metadata": {
        "id": "9cR5D1zwRimY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible models that we can try : Transducteurs (seq2seq with attention or transformers)\n",
        "\n",
        "T5-small / T5-base\n",
        "\n",
        "FLAN-T5-small\n",
        "\n",
        "BART or distilBART\n",
        "\n",
        "ByT5 : specialisez in byte-level treatment -> usefull for OCR errors.\n",
        "\n",
        "Charformer / CANINE : char-level to get thine level of granularity\n",
        "\n",
        "🔹 Baselines :\n",
        "Levenshtein-based correction\n",
        "\n",
        "Spell checkers + N-grams (as Hunspell or SymSpell with context)"
      ],
      "metadata": {
        "id": "5kIKK8naVjJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sentencepiece --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuH4cZR9WNEP",
        "outputId": "80841291-ff28-44c9-cc77-41ed1bd4f0dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5 : Model fine tuned for grammar"
      ],
      "metadata": {
        "id": "r8WgDq5dWNVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5-base"
      ],
      "metadata": {
        "id": "vkglRuwXFGCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a function to apply a prompt wich asks to correct the data, to the LLM T5-base, and print the output of this LLM"
      ],
      "metadata": {
        "id": "OxXoINVCz_As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Charge model and tokenizer\n",
        "model_name = \"t5-base\" # We could use also t5-small\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"Fix errors : {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=False, padding=False).to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "SrBGLqP7WWKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the firsts sentences of a noisy sentence\n",
        "noisy_text = ocr_data['0'][:1000]\n",
        "\n",
        "# Build segment, and prompt to correct on each segment\n",
        "segments = noisy_text.split('\\n')\n",
        "\n",
        "segments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyrHYk7-gnYl",
        "outputId": "63f5a608-bd9e-4cfe-b191-6a00229f9093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['THE VAMPYRE;',\n",
              " 'A Tale.',\n",
              " 'By John William Polidori',\n",
              " 'THEsuperstition upon which this taIe iſ founded is very general in the East. Among tho Arabjans itappeors to be common: it did not, however, extend itself to the Greeks until after the establi shment of Christianity; and it has only aſsumed its prosent form since the division af the Latin and Greok churches; at which time, lhe idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, it gradually increosed, and formed lhe subject of many wonderful stories, ſtill extant, of the dead rising from their graves, and feeding uponlhe blood of tho young and beautiful. In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of c0nsumptions; whilst these human blood-suckers fattened—and their veins became distend']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the model on a small dataset\n",
        "for i in range(4):\n",
        "  print(correct_text_with_t5(segments[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vd1Ra8_zOBW",
        "outputId": "065cb9c4-3241-44ec-c66e-bf26853c7127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".: THE VAMPYRE; THE VAMPYRE; THE VAMPY\n",
            ": A Tale.\n",
            "Fix errors : By John William Polidori : By John William Polidori :\n",
            ". : Fix: Fix bug fixes : Fix errors : Fix bug fixes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model makes a lot of problem, as you can see on the transcription he proposed on the above cell. I tried to modify the prompt, the lenght of the inputs, and many other parameters, but still, the quality of the output data was too bad. We will not focus on this model, but look at another one instead : vennify/t5-base-grammar-correction"
      ],
      "metadata": {
        "id": "SRFikl1vzo3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5-base-grammar-correction"
      ],
      "metadata": {
        "id": "BqqkbdB5FL0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load fine-tuned grammar correction model\n",
        "model_name = \"vennify/t5-base-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test on a small dataset\n",
        "noisy_text = ocr_data['0'][:1000]\n",
        "segments = [s.strip() for s in noisy_text.split('\\n') if s.strip()]\n",
        "\n",
        "corrected_segments = [correct_text_with_t5(seg) for seg in segments]\n",
        "corrected_text = '\\n'.join(corrected_segments)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Oyi4f1jv-I",
        "outputId": "0c7c2c46-3d2c-48e5-9a95-143c7872b8a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE.\n",
            "A Tale.\n",
            "By John William Polidori.\n",
            "The superstition upon which this taIe is founded is very general in the East. It did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its prosent form since the division of the Latin and Greok churches; and it gradually increosed, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding upon the blood of young and beautiful people. In the West it spread, with some slight variation,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : The output is not completed. So we increase the max_lenghts to 512. Unfortunatly, it's not enough => we have no longer output for very high values of max_lengths, than with max_lenght = 512. We also try to remove early_stopping, but it does not work. So, as the model can not output very long sentences, we apply it many times, on slices of the text.\n",
        "\n",
        "- Instead of : model(sentence 1, sentence 2, sentence 3...)\n",
        "- We do : model(sentence 1) + model(sentence 2) + model(sentence 3) + ...\n",
        "\n",
        "We use spacy to slice into sentences"
      ],
      "metadata": {
        "id": "xZownBU5yBky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle T5 fine-tuné pour la correction grammaticale\n",
        "model_name = \"vennify/t5-base-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction pour corriger une phrase avec T5\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=512,\n",
        "        num_beams=4,\n",
        "        early_stopping=False,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Fonction principale : découpe par ligne, puis phrase, puis correction\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence) for sentence in sentences]\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR\n",
        "noisy_text = ocr_data['0'][:3000]  # ou le texte entier\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRlY6fqfwLpb",
        "outputId": "5e0018c6-5d3e-476f-d475-c46c8a88a2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE.\n",
            "A Tale.\n",
            "By John William Polidori.\n",
            "THE superstition upon which this theory is founded is very general in the East. It did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its prosent form since the division of the Latin and Greok churches; at which time, the idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, gradually increased, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding upon the blood of young and beautiful. In the West it spread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and quickly died of c0nsumptions; while these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as t0\n",
            "In the London Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to have been accurred at Madreyga, in Hungary. It appears, that upon an examination of the cornmander-in-chief arid magistrates of tbe place, they positively and unanimously affirmed, that, about five years before, a certairi Heyduke, named Arnold Paul, had bcen beclrd to say, that at Cassovia, ori the fr0ntiers of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid This precaution, however, did not prevent him from becoming a vampire; sor, about twenty or thirty days after his death and burial, many persons complained of having been tormented by him, and a deposition was made, that four persons had been deprived of life by his attacks. To prevent further mischief, the lnhabitants havjng consulted their Hadagni, took up the body, and found it (as is supposed to be usual in cafes of uampyrism) fresh, and entjrely free from corruption, and emitting at the rnouth, riose, and ears, pure and fIorid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely through the bear and body of Arnold Paul, at which he is reported to have cried out dreadfully as he had been olive. This done, they cut of his head, burned his body, and threw his body into his grave. The same measures were adopted with the corses of the other persons who had previously dicked from varnpyrism, lest they should, in their turn, become clgentf upan others who survived them.\n",
            "The universe1 belief js, that a person sucked by a vampyre becomes a vampire himself, and an arid sucks in his turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is he.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gzEGKERwopm",
        "outputId": "6f2293c8-d2b6-4c40-cc45-c68d4254cbd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2733"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_data_text[:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SV1NDt7w7KT",
        "outputId": "7a663fef-d9d8-49c6-ae24-59f3800f7edf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE VAMPYRE;\n",
            "A Tale.\n",
            "By John William Polidori\n",
            "THE superstition upon which this tale is founded is very general in the East. Among the Arabians it appears to be common: it did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its present form since the division of the Latin and Greek churches; at which time, the idea becoming prevalent, that a Latin body could not corrupt if buried in their territory, it gradually increased, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding upon the blood of the young and beautiful. In the West it spread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, where the belief existed, that vampyres nightly imbibed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of consumptions; whilst these human blood-suckers fattened—and their veins became distended to such a state of repletion, as to cause the blood to flow from all the passages of their bodies, and even from the very pores of their skins.\n",
            "In the London Journal, of March, 1732, is a curious, and, of course, credible account of a particular case of vampyrism, which is stated to have occurred at Madreyga, in Hungary. It appears, that upon an examination of the commander-in-chief and magistrates of the place, they positively and unanimously affirmed, that, about five years before, a certain Heyduke, named Arnold Paul, had been heard to say, that, at Cassovia, on the frontiers of the Turkish Servia, he had been tormented by a vampyre, but had found a way to rid himself of the evil, by eating some of the earth out of the vampyre's grave, and rubbing himself with his blood. This precaution, however, did not prevent him from becoming a vampyre himself; for, about twenty or thirty days after his death and burial, many persons complained of having been tormented by him, and a deposition was made, that four persons had been deprived of life by his attacks. To prevent further mischief, the inhabitants having consulted their Hadagni, took up the body, and found it (as is supposed to be usual in cases of vampyrism) fresh, and entirely free from corruption, and emitting at the mouth, nose, and ears, pure and florid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely through the heart and body of Arnold Paul, at which he is reported to have cried out as dreadfully as if he had been alive. This done, they cut off his head, burned his body, and threw the ashes into his grave. The same measures were adopted with the corses of those persons who had previously died from vampyrism, lest they should, in their turn, become agents upon others who survived them.\n",
            "The universal belief is, that a person sucked by a vampyre becomes a vampyre himself, and sucks in his turn.\n",
            "Chief bailiff.\n",
            "This monstrous rodomontade is here relat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : the model skip some parts of the text. We input 3000 characters, and it output only 2733. When looking into details at the translation, we see that it skips some sentences. Maybe, if we try to apply the LLM on smaller slices of the text, we won't skip parts. NOw, we slices when there is a '\\n', and when there is a ,:;!?"
      ],
      "metadata": {
        "id": "rLuogotVwOzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle T5 fine-tuné pour la correction grammaticale\n",
        "model_name = \"vennify/t5-base-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction pour corriger une phrase avec T5\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=1024,       # Allonge la sortie maximale (attention à la RAM GPU)\n",
        "        num_beams=4,\n",
        "        early_stopping=False, # Ne pas stopper la génération prématurément\n",
        "        length_penalty=1.0,   # Garde des sorties de taille naturelle\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "\n",
        "\n",
        "def split_sentences_and_punct(text):\n",
        "    # Split d'abord en phrases spaCy\n",
        "    spacy_sents = split_into_sentences_spacy(text)\n",
        "\n",
        "    # Ensuite split par ponctuation dans chaque phrase\n",
        "    punct_split_sents = []\n",
        "    for sent in spacy_sents:\n",
        "        # Split sur virgule, point d'exclamation, point d'interrogation, point-virgule, deux-points\n",
        "        parts = re.split(r'[,:;!?]', sent)\n",
        "        parts = [p.strip() for p in parts if p.strip()]\n",
        "        punct_split_sents.extend(parts)\n",
        "    return punct_split_sents\n",
        "\n",
        "\n",
        "# Fonction principale : découpe par ligne, puis phrase, puis correction\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_sentences_and_punct(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence) for sentence in sentences]\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR\n",
        "noisy_text = ocr_data['0'][:3000]  # ou le texte entier\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BvZrWGfr5RT",
        "outputId": "b079dd68-179d-4f3d-f9a8-b36505868e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE is correct.\n",
            "A Tale.\n",
            "By John William Polidori.\n",
            "THE superstition upon which this theory is founded is very general in the East. Among Arabjans itappeors to be common. It did not. However, this is correct: however, the facts are correct. It extends itself to the Greeks until after the establishment of Christianity. And it has only assumed its prosent form since the division of the Latin and Greok churches. At which time is correct: at which time. The idea is becoming prevalent. That a Lcltin body could not be corrvpl if buried in their territory. It gradually increased. And formed the subject of many wonderful stories. The fact is that there are still a lot of fossils that are still extant. The dead are rising from their graves. And feeding on the blood of tho young and beautiful. In the West it spreads throughout the world. With some slight variation. All over Hungary, Hungary is correct. Poland is correct. Austria is correct. And Lorraine is correct. Whoro the helies existed? Who the hells existed? That vompyresnightly imbi6ed a certain portion of the blood of their victims. Who became emaciated? They lost their strength. And quickly died of c0nsumptions. While these human blood-suckers fattened—and their veins became distended to such a state of ropletion. As t0 cause the blood to flow from all the passages of their badies. And even for the ucry pores of the skin.\n",
            "In theLond0n Journal. The month of March is correct. 1732, correct: 1732. Is a curiovs. Correct: and so on. Of course, of course. Credible account of a particular case of vampyrifin. Which is stated to have been accurred at Madreyga. In Hungary. It appears that it appears to be correct. Upon an examination of the cornmander-in-chief arid magistrates of the place, it is correct that upon examination of these judges, the judge is correct. They positively and unanimously affirmed that. That is correct: that is correct. About five years before that. A certairi Heyduke. Arnold Paul was named Arnold Paul. Had bcen beclrd to say that. That is correct: that is correct. At Cassovia. Or the descendants of the Turkish Servio are correct. He had been tormented by a vampire. But he had found a way to rid himself of the euj1. By eating some of the earth out of the vampyre's grove. And rubbing himself with his blood. This precaution is correct. However, this is correct: however, the facts are correct. Did not prevent him from bccoming a vampyre himsels. Sorrow: sorrow. About twenty or thirty days after his death and burial. Many persons complain of being tortured by him. And a deposition was made. Four persons had been deprived of life by his attacks. To prevent further mischief. The lnhabitants havjng consulted their Hadagni. Correct: took up the body. And f ound it (as is supposed to be usual in cafes of utopia) fresh. And entjrely free from corruption. Correct and emitting at the rearnouth. riose, you are correct. And ears are correct. Pure and healthy blood. Proof having been thus obtained, proof has been obtained. They resorted to the accustomed remedy. A stake was driven entirely through the bear and body of Arnold Paul. At which he is reported to have cried out dreadfully as is he had been olive. This is done. They cut of his head. He burned his body. And threw his asbes into his grave. The same measures were adopted with the cases of the same persons who had previously dicked from varnpyrism. Lest they should be correct. In their turn, they are correct. Become clgentf upan others who survived them.\n",
            "The universe1 belief js. That a person sucked by a vampyre becomes a vampire himself. Arid sucks in his turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is he.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ocr_data['0'][:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTMGMXN8qkRb",
        "outputId": "6738f9b4-cfec-4add-ef57-2a593de4221c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE VAMPYRE;\n",
            "A Tale.\n",
            "By John William Polidori\n",
            "THEsuperstition upon which this taIe iſ founded is very general in the East. Among tho Arabjans itappeors to be common: it did not, however, extend itself to the Greeks until after the establi shment of Christianity; and it has only aſsumed its prosent form since the division af the Latin and Greok churches; at which time, lhe idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, it gradually increosed, and formed lhe subject of many wonderful stories, ſtill extant, of the dead rising from their graves, and feeding uponlhe blood of tho young and beautiful. In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of c0nsumptions; whilst these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as t0 cause the blood to flow from all the passages of their badies, and even fr0m the ucry pores of thoir skins.\n",
            "In theLond0n Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to hove accurred at Madreyga, in Hungary. It appears, that upon an examination of the cornmander-in-chief arid magistrates of tbe place, they positively and unanimously affirmed, that, about five years before, a certairi Heyduke, named Arnold Paul, had bcen beclrd to say, that, at Cassovia, ori the fr0ntiers of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid himself of the euj1, by eating some of the earth out of the vampyre's grove, and rubbing himſelf with his blood. This prccaution, however, did not prevent him from bccoming a vampyre himſels; sor, about twenty or thirty days after his death and burial, many persons complainod of hauing 6een tormented by him, and a deposition was made, that four persons had been deprived os life by his attacks. To prevent further mischief, the lnhabitants havjng consulted their Hadagni, took up the body, and f ound it (aſ is supposed to be usual in cafes of uampyrism) fresh, and entjrely free from corruptjon, and emitting at the rnouth, riose, and ears, pure and fIorid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely lhrough the bearl and body of Arnold Paul, at which he is reported to hauecried out cls dreadfully as is he had been olive. This done, they cut ofs his head, burned his body, and threw lhe asbes into his grave. The same measures were adopted with the corses of th ose persons who had previously dicd from varnpyrism, lest they should, in theirturn, become clgentf upan others who survived them.\n",
            "The universa1 belief js, that a person sucked by a vampyre becomes a vampyre himself, arid sucks in his turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNSPUzTostFg",
        "outputId": "86d8330c-2474-4e63-fc1f-224590126885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3686"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New problems : the time computation starts beeing very high : 3 min to apply on 3000 characters. When we will generalize it to the 48 texts, it's going to take hours. Moreover, the outputs are not correct :    \n",
        "\n",
        "- Noisy : In theLond0n Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to hove accurred at Madreyga, in Hungary.\n",
        "\n",
        "- Cleaned : In theLond0n Journal. The month of March is correct. 1732, correct: 1732. Is a curiovs. Correct: and so on. Of course, of course. Credible account of a particular case of vampyrifin. Which is stated to have been accurred at Madreyga. In Hungary."
      ],
      "metadata": {
        "id": "TN3c7h57xGXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5-small-grammar-correction"
      ],
      "metadata": {
        "id": "hr9m9gA8FthM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce modèle est une version quantifiée en FP16 de t5-small, fine-tunée sur le jeu de données JFLEG pour la correction grammaticale. Il est optimisé pour une inférence rapide tout en maintenant une bonne précision.\n",
        "\n",
        "Différence avec t5-base-grammar-correction :\n",
        "\n",
        "t5-base-grammar-correction : 220M de paramètres, vitesse d'execution moyenne\n",
        "\n",
        "T5-small-grammar-correction : 60M de paramètres, vitesse d'execution plus rapide.\n",
        "\n",
        "Chaque modèle est parti de T5 (small ou base), puis a été fine tuné"
      ],
      "metadata": {
        "id": "vAAT-F6-Fyds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle T5 fine-tuné pour la correction grammaticale\n",
        "model_name = \"AventIQ-AI/T5-small-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction pour corriger une phrase avec T5\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=512,\n",
        "        num_beams=4,\n",
        "        early_stopping=False,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Fonction principale : découpe par ligne, puis phrase, puis correction\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence) for sentence in sentences]\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR\n",
        "noisy_text = ocr_data['0'][:3000]  # ou le texte entier\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJbW5un6Fx_M",
        "outputId": "2f63ae21-f0cb-4d9e-fb99-4c0e6eadd5a0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " е аее;\n",
            "A Tale.\n",
            "By John William Polidori\n",
            "The superstition upon which this taIe is founded is very general in the East. Among tho Arabjans itappeors to be common: it did not extend itself to the Greeks until after the establi shment of Christianity; and it has only assumed its prosent form since the division af the Latin and Greok churches; at which time, lhe idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, it gradually increosed, and formed a subject of many wonderful stories, still ex In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of c0nsumptions; while these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as\n",
            "In theLond0n Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to hove accurred at Madreyga, in Hungary. It appears, that upon an examination of the cornmander-in-chief arid magistrates of tbe place, they positively and unanimously affirmed, that, about five years before, a certairi Heyduke, named Arnold Paul, had bcen beclrd to say that, at Cassovia, ori the fr0ntiers of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid This prccaution, however, did not prevent him from bccoming a vampyre himsels; sor, about twenty or thirty days after his death and burial, many persons complain of being tormented by him, and a deposition was made, that four persons had been deprived os life by his attacks. To prevent further mischief, the lnhabitants havjng consulted their Hadagni, took up the body, and f ound it fresh, and free from corruptjon, and emitting at the rnouth, riose, and ears, pure and iorid blood. The proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely lhrough the bearl and body of Arnold Paul, at which he is reported to hauecried out cls dreadfully as is he had been olive. This done, they cut ofs his head, burned his body, and threw lhe asbes into his grave. The same measures were adopted with the corses of th ose persons who had previously dicd from varnpyrism, so that they should, in their turn, become clgentf upan others who survived them.\n",
            "The universa1 belief js, that a person sucked by a vampyre becomes a self, arid sucks in his turn.\n",
            "I am correct: Chlef bai1iff.\n",
            "This monstrous rodomontade is he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5-efficient-tiny-grammar-correction"
      ],
      "metadata": {
        "id": "hZ3Hy1GjK4Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On utilise un modèle encore plus petit que T5-small : T5-tiny"
      ],
      "metadata": {
        "id": "Sa330QVqK9WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle tiny pour la correction grammaticale\n",
        "model_name = \"visheratin/t5-efficient-tiny-grammar-correction\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction de correction d'une phrase\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text.strip()}\"  # ou juste `text.strip()` si prompt ne donne pas de gain\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    try:\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=128,\n",
        "            num_beams=2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur sur phrase : {text} — {e}\")\n",
        "        return text  # On retourne la phrase d'origine si erreur\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Pipeline principal\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence) for sentence in sentences]\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR (remplace ici par ton propre texte OCR)\n",
        "noisy_text = ocr_data['0'][:3000]  # ou texte brut\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307,
          "referenced_widgets": [
            "fc942b8237dc4f8eb1c3feaa742aa910",
            "d15e6eedeb5b4916be130701142add3d",
            "b289e01b8f86432e980506f560c337c6",
            "d5fb4926805b4e01bf4a3542169df15f",
            "ed6df9b6131f427fa8a013ec78b76091",
            "a0f4ada18ad44603a50356f4030b87ab",
            "03f388ae10e446bd9a1999b09a7b4b51",
            "2809a1ca55b94384adff6fd8e7391550",
            "0a40d0a10af040549b8383b21d474b0c",
            "f74e02ea166445a3a6fa34f052a49f0a",
            "241f389b67cf4c83938edd7e112b7a77",
            "08e88f65803844a0b615a8370c8dee1e",
            "fcaef48f103c4b2da6aab082cba4ce79",
            "aa89cdada5fd4baab5a317926b8e9735",
            "51fffdf0888146359dc64afb6f68c1a4",
            "2f512852a1cd45298570639f2858eb73",
            "b731ae7fc2824af880e7d6f8e93f7948",
            "23191f826e4942d595f5046922d80fd8",
            "ff57d1b9ff0a4ca9b1221245eca4776d",
            "bf4e816de7224b84add1fb422f75770f",
            "2d970982bb36415f94878b1ade043781",
            "5c22fa74557948359bf175f14059dc2e",
            "de4a464ea1dd40abb99a8787819269c5",
            "c11b5478f80142b289a49e968658f8ed",
            "c51cddd3b4314183a8aa781d838fa0ee",
            "eee95196c3cd4306875547c6d3b1527a",
            "e3bc914458b7443fa9e77fdc399c81bd",
            "669467b4e1414673bc32328ce5b986a2",
            "7329a566985e40d6951acf0eb6cc5529",
            "39b6df90c9794a4daf71122240f9db61",
            "d510188131ef443cad2b34fd5697eb3f",
            "ea93061fa70d43eda8ca7e338067d02f",
            "f81d49eecd774465bb22aa971a99a705"
          ]
        },
        "id": "l2ulkEFrK8R0",
        "outputId": "60429bea-2eb1-43fb-dfd5-da74589d3627"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc942b8237dc4f8eb1c3feaa742aa910"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/62.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08e88f65803844a0b615a8370c8dee1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/62.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de4a464ea1dd40abb99a8787819269c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " correct: THE VAMPYRE;\n",
            "The correct: A Tale.\n",
            "correct: By John William Polidori.\n",
            "correct: THE superstition upon which this site is founded is very general in the East. correct: Among the Arabjans itappeors to be common: it did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its present form since the division of Latin and Greok churches; at which time, the idea of becoming prevalent, that a Latin body could not correct if buried in their territory, it gradually increased, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding uponlhe blood of tho young and beautiful. Correct: In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbibed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of consumption; whilst these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as to cause the blood to flow from all the passages of their badies, and even from\n",
            "correct: In the London Journal, of March ,1732, is a curious, and, of course, credible account of a particular case of vampyrifin, which is stated to have accurred at Madreyga, in Hungary. Correct: It appears that upon an examination of the cornmander-in-chief arid magistrates of this place, they positively and unanimously affirmed that, about five years before, a certairi Heyduke, named Arnold Paul, had been referred to say, that, at Cassovia, ori the friends of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid himself of the euj 1, by eating some of the earth out of the vampyres grove, correct: This proposition, however, did not prevent him from becoming a vampyre himself; sor, about twenty or thirty days after his death and burial, many people complained of having been tormented by him, and a deposition was made, that four people had been deprived of life by his attacks. correct: To prevent further mischief, the inhabitants having consulted their Hadagni, took up the body, and found it (as is supposed to be usual in cafes of uampyrism) fresh, and entirely free from corruption, and emitting at the rnouth, riose, and ears, pure and foorid blood. correct: Proof having been thus obtained, they resorted to the accustomed remedy. correct: A stake was driven entirely through the bearl and body of Arnold Paul, at which he is reported to hauecried out colds dreadfully as he had been olive. correct: This done, they cut off his head, burned his body, and threw the bases into his grave. correct: The same measures were adopted with the course of those people who had previously decided from a varnpyrism, lest they should, in their turn, become confident than others who survived them.\n",
            "correct: The universal belief just that a person sucked by a vampyre becomes a vampyre himself, arid sucks in his turn.\n",
            "correct: Chlef, a bailiff.\n",
            "correct: This monstrous rodomontade is here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : At the begining of each correction, we have the word 'Correct: '. But as it is the same for each sentences that has been corrected, we just have to remove it !"
      ],
      "metadata": {
        "id": "1bRdyEBWNyBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle tiny pour la correction grammaticale\n",
        "model_name = \"visheratin/t5-efficient-tiny-grammar-correction\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction de correction d'une phrase\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text.strip()}\"  # ou juste `text.strip()` si prompt ne donne pas de gain\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    try:\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=128,\n",
        "            num_beams=2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur sur phrase : {text} — {e}\")\n",
        "        return text  # On retourne la phrase d'origine si erreur\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Pipeline principal\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence)[9:] for sentence in sentences] # it removes the part 'correct: '\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR (remplace ici par ton propre texte OCR)\n",
        "noisy_text = ocr_data['0'][:3000]  # ou texte brut\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuQOhY-3OAAP",
        "outputId": "875f5598-6507-49b6-80b0-80de3870b4d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE;\n",
            "ct: A Tale.\n",
            "By John William Polidori.\n",
            "THE superstition upon which this site is founded is very general in the East. Among the Arabjans itappeors to be common: it did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its present form since the division of Latin and Greok churches; at which time, the idea of becoming prevalent, that a Latin body could not correct if buried in their territory, it gradually increased, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding uponlhe blood of tho young and beautiful. In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbibed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of consumption; whilst these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as to cause the blood to flow from all the passages of their badies, and even from\n",
            "In the London Journal, of March ,1732, is a curious, and, of course, credible account of a particular case of vampyrifin, which is stated to have accurred at Madreyga, in Hungary. It appears that upon an examination of the cornmander-in-chief arid magistrates of this place, they positively and unanimously affirmed that, about five years before, a certairi Heyduke, named Arnold Paul, had been referred to say, that, at Cassovia, ori the friends of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid himself of the euj 1, by eating some of the earth out of the vampyres grove, This proposition, however, did not prevent him from becoming a vampyre himself; sor, about twenty or thirty days after his death and burial, many people complained of having been tormented by him, and a deposition was made, that four people had been deprived of life by his attacks. To prevent further mischief, the inhabitants having consulted their Hadagni, took up the body, and found it (as is supposed to be usual in cafes of uampyrism) fresh, and entirely free from corruption, and emitting at the rnouth, riose, and ears, pure and foorid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely through the bearl and body of Arnold Paul, at which he is reported to hauecried out colds dreadfully as he had been olive. This done, they cut off his head, burned his body, and threw the bases into his grave. The same measures were adopted with the course of those people who had previously decided from a varnpyrism, lest they should, in their turn, become confident than others who survived them.\n",
            "The universal belief just that a person sucked by a vampyre becomes a vampyre himself, arid sucks in his turn.\n",
            "Chlef, a bailiff.\n",
            "This monstrous rodomontade is here.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Translation"
      ],
      "metadata": {
        "id": "HymXLsnzQnH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we pass the model through a machine translation, to have the french text, and put it back to english, to see if machine translation are capable to correct OCR mistakes"
      ],
      "metadata": {
        "id": "LypvzAIbQz39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MarianMTModel"
      ],
      "metadata": {
        "id": "ZBKYJnJdaUMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "def translate(text, src_lang, tgt_lang):\n",
        "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    translated = model.generate(**inputs)\n",
        "    return tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "# Texte source en anglais\n",
        "original_text = \"This document was extracted from a noisy OCR scan. It needs to be corrected and translated.\"\n",
        "print(\"📘Original text :\", original_text)\n",
        "\n",
        "# Étape 1 : Traduire en français\n",
        "translated_to_french = translate(original_text, \"en\", \"fr\")\n",
        "print(\"➡️ Traduction en français :\\n\", translated_to_french)\n",
        "\n",
        "# Étape 2 : Revenir à l’anglais\n",
        "translated_back_to_english = translate(translated_to_french, \"fr\", \"en\")\n",
        "print(\"\\n⬅️ Traduction de retour en anglais :\\n\", translated_back_to_english)"
      ],
      "metadata": {
        "id": "GJNBU7KmRwz3",
        "outputId": "cb580fe2-be64-4db2-a353-24f5073f033e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘Original text : This document was extracted from a noisy OCR scan. It needs to be corrected and translated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:177: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "➡️ Traduction en français :\n",
            " Ce document a été extrait d'un scanner OCR bruyant. Il doit être corrigé et traduit.\n",
            "\n",
            "⬅️ Traduction de retour en anglais :\n",
            " This document has been extracted from a noisy OCR scanner. It must be corrected and translated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything works, so now, we apply it on our dataset :"
      ],
      "metadata": {
        "id": "zq0uLClXUFxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Chargement du modèle de segmentation en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Modèle EN → FR\n",
        "en_fr_model_name = \"Helsinki-NLP/opus-mt-en-fr\"\n",
        "en_fr_tokenizer = AutoTokenizer.from_pretrained(en_fr_model_name)\n",
        "en_fr_model = AutoModelForSeq2SeqLM.from_pretrained(en_fr_model_name)\n",
        "\n",
        "# Modèle FR → EN\n",
        "fr_en_model_name = \"Helsinki-NLP/opus-mt-fr-en\"\n",
        "fr_en_tokenizer = AutoTokenizer.from_pretrained(fr_en_model_name)\n",
        "fr_en_model = AutoModelForSeq2SeqLM.from_pretrained(fr_en_model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "en_fr_model.to(device)\n",
        "fr_en_model.to(device)\n",
        "\n",
        "# Fonction générique de traduction\n",
        "def translate(text, tokenizer, model, src_lang=\"en\", tgt_lang=\"fr\"):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    try:\n",
        "        outputs = model.generate(**inputs, max_length=512, num_beams=4, early_stopping=True)\n",
        "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de traduction {src_lang}→{tgt_lang} : {text} — {e}\")\n",
        "        return text\n",
        "\n",
        "# Segmentation du texte\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Pipeline de traduction et back-traduction\n",
        "def translate_and_backtranslate_text(text):\n",
        "    lines = text.split('\\n')\n",
        "    final_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            final_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        processed_sentences = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            fr = translate(sent, en_fr_tokenizer, en_fr_model, \"en\", \"fr\")\n",
        "            back_en = translate(fr, fr_en_tokenizer, fr_en_model, \"fr\", \"en\")\n",
        "            processed_sentences.append(back_en)\n",
        "\n",
        "        final_line = ' '.join(processed_sentences)\n",
        "        final_lines.append(final_line)\n",
        "\n",
        "    return '\\n'.join(final_lines)\n",
        "\n",
        "noisy_text = ocr_data['0'][:3000]  # ou texte brut\n",
        "translated_and_back = translate_and_backtranslate_text(noisy_text)\n",
        "\n",
        "print(\"🔁 Back-Translated Output:\\n\", translated_and_back)"
      ],
      "metadata": {
        "id": "VfD-5TV5ThqO",
        "outputId": "2b462499-1bb6-4fd9-ee4b-f33892667478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Back-Translated Output:\n",
            " VAMPYRE;\n",
            "A tale.\n",
            "By John William Polidori\n",
            "THE superstition on which this tae is based is very general in the East. However, it did not extend to the Greeks until after the establishment of Christianity; and it did not take the form of prosents since the division that the Latin and Greek churches had; at that time, the idea of becoming dominant, that a body of Lcltin could not corrupt if it was buried in their territory, it gradually creed, and formed the object of many wonderful stories, still existing, of the dead who rose from their graves, and fed on the blood of young and beautiful tho. In the West, it spread, with a slight variation, throughout Hungary, Poland, Austria and Lorraine, which were the helies, that part of the blood of their victims, which has become emaciated, lost their strength and quickly died of c0nomptions; while these human blood suckers have become fattened — and their veins have become distended to such a state of ropletion, as t0 cause the blood of all passages of their badges to flow, and even fr0m the ucry pores of thoir skins.\n",
            "In the London Journal of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is declared for Hove accused in Madreyga, Hungary. It seems that, in an examination of the arid magistrates of the chief impoundment, they affirmed in a positive and unanimous way, that about five years earlier, a certairi Heyduke, named Arnold Paul, had blotted out to say, that, in Cassova, ori the brothers of the Turkish Servio, he had been tormented by a vampyr, but had found a way to get rid of the euj1, by eating part of the earth of the epi of the vampyr, and rubbing himself with his blood. This precaution, however, did not prevent him from flouting a vampyre hesels; so, about twenty or thirty days after his death and burial, many people complain of hauing 6th tormented by him, and a statement was made, that four people had been deprived of life by his attacks. In order to avoid other troubles, the inhabitants consulted their Hadagni, took the body, and did it freshly (as it is supposed to be in the cafes of uampyrism) and fully free of corruptionjon, and emit to rnuth, riosis, and ears, pure and fIorid blood. As the evidence was obtained, they resorted to the usual remedy. A stake was entirely pushed by Arnold Paul's bear and body, where it is reported to haecreed cls terribly as it was olive. They cut off his head, burned his body, and threw asbes into his grave. The same measures were adopted with the horns of these people who had previously dictated varpyrism, for fear that they would in turn become undisputed by others who survived them.\n",
            "The universe1 belief js, that a person sucked by a vampire becomes a vampire himself, arid sucks in turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Facebook"
      ],
      "metadata": {
        "id": "9ocG3OzIaaeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Chargement de spacy pour la segmentation\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Modèle NLLB\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)  # <-- Important\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Codes de langue NLLB\n",
        "lang_code = {\n",
        "    \"en\": \"eng_Latn\",\n",
        "    \"fr\": \"fra_Latn\"\n",
        "}\n",
        "\n",
        "# Fonction de traduction NLLB\n",
        "def translate_nllb(text, tokenizer, model, src_lang=\"en\", tgt_lang=\"fr\"):\n",
        "    try:\n",
        "        tokenizer.src_lang = lang_code[src_lang]\n",
        "        encoded = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        generated_tokens = model.generate(\n",
        "            **encoded,\n",
        "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(lang_code[tgt_lang]),  # <-- fix\n",
        "            max_length=512,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de traduction {src_lang}→{tgt_lang} : {text} — {e}\")\n",
        "        return text\n",
        "\n",
        "# Segmentation des phrases\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Pipeline complet\n",
        "def translate_and_backtranslate_nllb(text):\n",
        "    lines = text.split('\\n')\n",
        "    final_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            final_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        processed_sentences = []\n",
        "\n",
        "        for sent in sentences:\n",
        "            fr = translate_nllb(sent, tokenizer, model, \"en\", \"fr\")\n",
        "            back_en = translate_nllb(fr, tokenizer, model, \"fr\", \"en\")\n",
        "            processed_sentences.append(back_en)\n",
        "\n",
        "        final_line = ' '.join(processed_sentences)\n",
        "        final_lines.append(final_line)\n",
        "\n",
        "    return '\\n'.join(final_lines)\n",
        "\n",
        "# Exemple d'utilisation\n",
        "noisy_text = ocr_data['0'][:3000]  # ou texte brut\n",
        "translated_and_back = translate_and_backtranslate_nllb(noisy_text)\n",
        "print(\"🔁 Back-Translated Output (NLLB):\\n\", translated_and_back)"
      ],
      "metadata": {
        "id": "auZP31uFab-f",
        "outputId": "7f4234cf-feb3-4a79-a967-408091d28296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔁 Back-Translated Output (NLLB):\n",
            " The vampire.\n",
            "It's a story.\n",
            "By John William Polidori\n",
            "The superstition on which this story is based is widespread in the East. Among the Arabs, it seems to be common: it did not, however, spread to the Greeks until after the establishment of Christianity; and it took its form only from the division of the Latin and Greek churches; at that time, as the idea became widespread, that a Lcltin body could not be corrupted if it was buried on their territory, it gradually became believed, and formed the subject of many wonderful stories, still existing, of the dead rising from their graves, and feeding on the blood of young and beautiful. In the West, it spread, with a slight variation, throughout Hungary, Poland, Austria, and Lorraine, where healers existed, that vampires absorbed at night a certain portion of the blood of their victims, who became emancipated, lost their strength, and died quickly of constipation; while these suckers of human blood grew fat and their veins extended to such a state of replication that they caused blood to flow from all the passages of their rings, and even from the pores of their skin.\n",
            "In the London Journal of March 1732, there is a curious and, of course, credible account of a particular case of vampirefin, which was reported in Madreyga, Hungary. It seems that after an examination by the commander-in-chief of the arid magistrates of Tbe Place, they affirmed positively and unanimously, that about five years earlier, a certain Heyduke, named Arnold Paul, had been commissioned to say, that, in Cassovia, or the first of the Turkish Servio, he had been tormented by a vampire, but had found a way to get rid of the egg1, by eating part of the land of the vampire forest, and rubbing himself with its blood. This caution, however, did not prevent him from being killed by a vampire; however, about twenty or thirty days after his death and burial, many people complained of being tortured by him, and a statement was made that four people had been deprived of their lives by his attacks. To avoid further evils, the inhabitants consulted their Hadagni, took the body and found it (as is customary in the coffeehouses of the umpirism) fresh and completely free of corruption, and emitting to the rnouth, riot, and ears, pure and ferocious blood. Having obtained the evidence, they resorted to the usual remedy. A stake was led entirely through the bear and the body of Arnold Paul, during which he would have cried terribly as if it had been an olive tree. They cut off his head and burned it and threw the ashes into his tomb. The same measures were taken for the hearts of those who had already left varnpyrism, lest they, in turn, become the keys to the other survivors.\n",
            "The universal belief that a person sucked in by a vampire becomes himself a vampire, in turn, is null.\n",
            "I beg of you.\n",
            "He 's the monstrous rider .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : It took 10 min to run just this few part of the code..."
      ],
      "metadata": {
        "id": "fxyZn6O7nEnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "xT860nzIFSg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "!pip install jiwer --quiet\n",
        "from jiwer import wer, cer\n",
        "\n",
        "total_cer = 0\n",
        "total_wer = 0\n",
        "\n",
        "for i in range(48):\n",
        "    ref = clean_data_text[i]\n",
        "    hyp = results[str(i)]\n",
        "    total_cer += cer(ref, hyp)\n",
        "    total_wer += wer(ref, hyp)\n",
        "\n",
        "print(f\"Mean CER: {total_cer / 48:.4f}\")\n",
        "print(f\"Mean WER: {total_wer / 48:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "AMfkb5L1XF1H",
        "outputId": "b564d2a6-98a5-4e0f-d192-44b563510039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/3.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/3.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-051abed6f5cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtotal_cer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_wer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/measures.py\u001b[0m in \u001b[0;36mcer\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform, return_dict, truth, truth_transform)\u001b[0m\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     output = process_characters(\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/process.py\u001b[0m in \u001b[0;36mprocess_characters\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# it's the same as word processing, just every word is of length 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     result = process_words(\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/process.py\u001b[0m in \u001b[0;36mprocess_words\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# pre-process reference and hypothesis by applying transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     ref_transformed = _apply_transform(\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_reference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/process.py\u001b[0m in \u001b[0;36m_apply_transform\u001b[0;34m(sentence, transform, is_reference)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mtransformed_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_non_empty_lists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         ):\n\u001b[0;32m--> 369\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0;34m\"After applying the transformation, each reference should be a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0;34m\"non-empty list of strings, with each string being a single word.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas"
      ],
      "metadata": {
        "id": "H5aPvLoGX0R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible ideas : get some data from the internet, pass it through an OCR, get the noisy text, and use it to finetune a model ?"
      ],
      "metadata": {
        "id": "WeGMTxI9Xny2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Possible idea : Pass multiple time the input through the model ? We hope that at first loop, it will improve the quality of text, thanks to eliminate evident mistakes, and with second, third etc... loop, it will eliminate less evident mistakes**"
      ],
      "metadata": {
        "id": "X8xz8e2vJ-Le"
      }
    }
  ]
}