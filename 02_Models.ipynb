{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQeauoABZpi2QHwlTe+bqC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tgarnier067/MNLP-project-2/blob/main/02_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "ngG6cCRETRoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Vqml8T0sB5Dw",
        "outputId": "0ff3b7de-4e98-41af-ee5a-0382370dd28a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-65e621a1-6c68-437a-9e6e-5bcbddb626c9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-65e621a1-6c68-437a-9e6e-5bcbddb626c9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving the_vampyre_clean.json to the_vampyre_clean.json\n",
            "Saving the_vampyre_ocr.json to the_vampyre_ocr.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"the_vampyre_clean.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    clean_data = json.load(f)\n",
        "\n",
        "with open(\"the_vampyre_ocr.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ocr_data = json.load(f)"
      ],
      "metadata": {
        "id": "l2_61Rw7CzvQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "K3gEh-A-TdxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_values_dict(d):\n",
        "    \"\"\"\n",
        "    Concat values of a dict, seperating each element with '\\n'\n",
        "\n",
        "    Args:\n",
        "        d (dict): Dictionnary\n",
        "\n",
        "    Returns:\n",
        "        str: concatenated text\n",
        "    \"\"\"\n",
        "    return '\\n'.join(d.get(str(i), \"\") for i in range(48))\n",
        "\n",
        "clean_data_text = concat_values_dict(clean_data)"
      ],
      "metadata": {
        "id": "9cR5D1zwRimY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible models that we can try : Transducteurs (seq2seq with attention or transformers)\n",
        "\n",
        "T5-small / T5-base\n",
        "\n",
        "FLAN-T5-small\n",
        "\n",
        "BART or distilBART\n",
        "\n",
        "ByT5 : specialisez in byte-level treatment -> usefull for OCR errors.\n",
        "\n",
        "Charformer / CANINE : char-level to get thine level of granularity\n",
        "\n",
        "🔹 Baselines :\n",
        "Levenshtein-based correction\n",
        "\n",
        "Spell checkers + N-grams (as Hunspell or SymSpell with context)"
      ],
      "metadata": {
        "id": "5kIKK8naVjJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T5"
      ],
      "metadata": {
        "id": "r8WgDq5dWNVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch sentencepiece --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuH4cZR9WNEP",
        "outputId": "c819d5a1-1dec-43b9-b2b8-dc0f395f3331"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a function to apply a prompt wich asks to correct the data, to the LLM T5-base, and print the output of this LLM"
      ],
      "metadata": {
        "id": "OxXoINVCz_As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Charge model and tokenizer\n",
        "model_name = \"t5-base\" # We could use also t5-small\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"Fix errors : {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=False, padding=False).to(device)\n",
        "    outputs = model.generate(**inputs)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "SrBGLqP7WWKW"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the firsts sentences of a noisy sentence\n",
        "noisy_text = ocr_data['0'][:1000]\n",
        "\n",
        "# Build segment, and prompt to correct on each segment\n",
        "segments = noisy_text.split('\\n')\n",
        "\n",
        "segments"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyrHYk7-gnYl",
        "outputId": "63f5a608-bd9e-4cfe-b191-6a00229f9093"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['THE VAMPYRE;',\n",
              " 'A Tale.',\n",
              " 'By John William Polidori',\n",
              " 'THEsuperstition upon which this taIe iſ founded is very general in the East. Among tho Arabjans itappeors to be common: it did not, however, extend itself to the Greeks until after the establi shment of Christianity; and it has only aſsumed its prosent form since the division af the Latin and Greok churches; at which time, lhe idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, it gradually increosed, and formed lhe subject of many wonderful stories, ſtill extant, of the dead rising from their graves, and feeding uponlhe blood of tho young and beautiful. In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of c0nsumptions; whilst these human blood-suckers fattened—and their veins became distend']"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the model on a small dataset\n",
        "for i in range(4):\n",
        "  print(correct_text_with_t5(segments[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vd1Ra8_zOBW",
        "outputId": "065cb9c4-3241-44ec-c66e-bf26853c7127"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".: THE VAMPYRE; THE VAMPYRE; THE VAMPY\n",
            ": A Tale.\n",
            "Fix errors : By John William Polidori : By John William Polidori :\n",
            ". : Fix: Fix bug fixes : Fix errors : Fix bug fixes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model makes a lot of problem, as you can see on the transcription he proposed on the above cell. I tried to modify the prompt, the lenght of the inputs, and many other parameters, but still, the quality of the output data was too bad. We will not focus on this model, but look at another one instead : vennify/t5-base-grammar-correction"
      ],
      "metadata": {
        "id": "SRFikl1vzo3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load fine-tuned grammar correction model\n",
        "model_name = \"vennify/t5-base-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Test on a small dataset\n",
        "noisy_text = ocr_data['0'][:1000]\n",
        "segments = [s.strip() for s in noisy_text.split('\\n') if s.strip()]\n",
        "\n",
        "corrected_segments = [correct_text_with_t5(seg) for seg in segments]\n",
        "corrected_text = '\\n'.join(corrected_segments)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0Oyi4f1jv-I",
        "outputId": "0c7c2c46-3d2c-48e5-9a95-143c7872b8a0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE.\n",
            "A Tale.\n",
            "By John William Polidori.\n",
            "The superstition upon which this taIe is founded is very general in the East. It did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its prosent form since the division of the Latin and Greok churches; and it gradually increosed, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding upon the blood of young and beautiful people. In the West it spread, with some slight variation,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : The output is not completed. So we increase the max_lenghts to 512. Unfortunatly, it's not enough => we have no longer output for very high values of max_lengths, than with max_lenght = 512. We also try to remove early_stopping, but it does not work. So, as the model can not output very long sentences, we apply it many times, on slices of the text.\n",
        "\n",
        "- Instead of : model(sentence 1, sentence 2, sentence 3...)\n",
        "- We do : model(sentence 1) + model(sentence 2) + model(sentence 3) + ...\n",
        "\n",
        "We use spacy to slice into sentences"
      ],
      "metadata": {
        "id": "xZownBU5yBky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import spacy\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle T5 fine-tuné pour la correction grammaticale\n",
        "model_name = \"vennify/t5-base-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction pour corriger une phrase avec T5\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=512,\n",
        "        num_beams=4,\n",
        "        early_stopping=False,\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "# Fonction principale : découpe par ligne, puis phrase, puis correction\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_into_sentences_spacy(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence) for sentence in sentences]\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR\n",
        "noisy_text = ocr_data['0'][:3000]  # ou le texte entier\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRlY6fqfwLpb",
        "outputId": "5e0018c6-5d3e-476f-d475-c46c8a88a2fc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE.\n",
            "A Tale.\n",
            "By John William Polidori.\n",
            "THE superstition upon which this theory is founded is very general in the East. It did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its prosent form since the division of the Latin and Greok churches; at which time, the idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, gradually increased, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding upon the blood of young and beautiful. In the West it spread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and quickly died of c0nsumptions; while these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as t0\n",
            "In the London Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to have been accurred at Madreyga, in Hungary. It appears, that upon an examination of the cornmander-in-chief arid magistrates of tbe place, they positively and unanimously affirmed, that, about five years before, a certairi Heyduke, named Arnold Paul, had bcen beclrd to say, that at Cassovia, ori the fr0ntiers of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid This precaution, however, did not prevent him from becoming a vampire; sor, about twenty or thirty days after his death and burial, many persons complained of having been tormented by him, and a deposition was made, that four persons had been deprived of life by his attacks. To prevent further mischief, the lnhabitants havjng consulted their Hadagni, took up the body, and found it (as is supposed to be usual in cafes of uampyrism) fresh, and entjrely free from corruption, and emitting at the rnouth, riose, and ears, pure and fIorid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely through the bear and body of Arnold Paul, at which he is reported to have cried out dreadfully as he had been olive. This done, they cut of his head, burned his body, and threw his body into his grave. The same measures were adopted with the corses of the other persons who had previously dicked from varnpyrism, lest they should, in their turn, become clgentf upan others who survived them.\n",
            "The universe1 belief js, that a person sucked by a vampyre becomes a vampire himself, and an arid sucks in his turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is he.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gzEGKERwopm",
        "outputId": "6f2293c8-d2b6-4c40-cc45-c68d4254cbd0"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2733"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_data_text[:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SV1NDt7w7KT",
        "outputId": "7a663fef-d9d8-49c6-ae24-59f3800f7edf"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE VAMPYRE;\n",
            "A Tale.\n",
            "By John William Polidori\n",
            "THE superstition upon which this tale is founded is very general in the East. Among the Arabians it appears to be common: it did not, however, extend itself to the Greeks until after the establishment of Christianity; and it has only assumed its present form since the division of the Latin and Greek churches; at which time, the idea becoming prevalent, that a Latin body could not corrupt if buried in their territory, it gradually increased, and formed the subject of many wonderful stories, still extant, of the dead rising from their graves, and feeding upon the blood of the young and beautiful. In the West it spread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, where the belief existed, that vampyres nightly imbibed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of consumptions; whilst these human blood-suckers fattened—and their veins became distended to such a state of repletion, as to cause the blood to flow from all the passages of their bodies, and even from the very pores of their skins.\n",
            "In the London Journal, of March, 1732, is a curious, and, of course, credible account of a particular case of vampyrism, which is stated to have occurred at Madreyga, in Hungary. It appears, that upon an examination of the commander-in-chief and magistrates of the place, they positively and unanimously affirmed, that, about five years before, a certain Heyduke, named Arnold Paul, had been heard to say, that, at Cassovia, on the frontiers of the Turkish Servia, he had been tormented by a vampyre, but had found a way to rid himself of the evil, by eating some of the earth out of the vampyre's grave, and rubbing himself with his blood. This precaution, however, did not prevent him from becoming a vampyre himself; for, about twenty or thirty days after his death and burial, many persons complained of having been tormented by him, and a deposition was made, that four persons had been deprived of life by his attacks. To prevent further mischief, the inhabitants having consulted their Hadagni, took up the body, and found it (as is supposed to be usual in cases of vampyrism) fresh, and entirely free from corruption, and emitting at the mouth, nose, and ears, pure and florid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely through the heart and body of Arnold Paul, at which he is reported to have cried out as dreadfully as if he had been alive. This done, they cut off his head, burned his body, and threw the ashes into his grave. The same measures were adopted with the corses of those persons who had previously died from vampyrism, lest they should, in their turn, become agents upon others who survived them.\n",
            "The universal belief is, that a person sucked by a vampyre becomes a vampyre himself, and sucks in his turn.\n",
            "Chief bailiff.\n",
            "This monstrous rodomontade is here relat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem : the model skip some parts of the text. We input 3000 characters, and it output only 2733. When looking into details at the translation, we see that it skips some sentences. Maybe, if we try to apply the LLM on smaller slices of the text, we won't skip parts. NOw, we slices when there is a '\\n', and when there is a ,:;!?"
      ],
      "metadata": {
        "id": "rLuogotVwOzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import spacy\n",
        "import re\n",
        "\n",
        "# Charger spaCy pour le découpage en phrases\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Charger le modèle T5 fine-tuné pour la correction grammaticale\n",
        "model_name = \"vennify/t5-base-grammar-correction\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Fonction pour corriger une phrase avec T5\n",
        "def correct_text_with_t5(text):\n",
        "    prompt = f\"correct: {text}\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=1024,       # Allonge la sortie maximale (attention à la RAM GPU)\n",
        "        num_beams=4,\n",
        "        early_stopping=False, # Ne pas stopper la génération prématurément\n",
        "        length_penalty=1.0,   # Garde des sorties de taille naturelle\n",
        "        no_repeat_ngram_size=3\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Découper un bloc de texte en phrases avec spaCy\n",
        "def split_into_sentences_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "\n",
        "\n",
        "def split_sentences_and_punct(text):\n",
        "    # Split d'abord en phrases spaCy\n",
        "    spacy_sents = split_into_sentences_spacy(text)\n",
        "\n",
        "    # Ensuite split par ponctuation dans chaque phrase\n",
        "    punct_split_sents = []\n",
        "    for sent in spacy_sents:\n",
        "        # Split sur virgule, point d'exclamation, point d'interrogation, point-virgule, deux-points\n",
        "        parts = re.split(r'[,:;!?]', sent)\n",
        "        parts = [p.strip() for p in parts if p.strip()]\n",
        "        punct_split_sents.extend(parts)\n",
        "    return punct_split_sents\n",
        "\n",
        "\n",
        "# Fonction principale : découpe par ligne, puis phrase, puis correction\n",
        "def correct_text_by_line_and_sentence(text):\n",
        "    lines = text.split('\\n')\n",
        "    corrected_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if not line.strip():\n",
        "            corrected_lines.append('')\n",
        "            continue\n",
        "\n",
        "        sentences = split_sentences_and_punct(line)\n",
        "        corrected_sentences = [correct_text_with_t5(sentence) for sentence in sentences]\n",
        "        corrected_line = ' '.join(corrected_sentences)\n",
        "        corrected_lines.append(corrected_line)\n",
        "\n",
        "    return '\\n'.join(corrected_lines)\n",
        "\n",
        "# Exemple avec texte OCR\n",
        "noisy_text = ocr_data['0'][:3000]  # ou le texte entier\n",
        "corrected_text = correct_text_by_line_and_sentence(noisy_text)\n",
        "\n",
        "print(\"Corrected Output:\\n\", corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BvZrWGfr5RT",
        "outputId": "b079dd68-179d-4f3d-f9a8-b36505868e26"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected Output:\n",
            " THE VAMPYRE is correct.\n",
            "A Tale.\n",
            "By John William Polidori.\n",
            "THE superstition upon which this theory is founded is very general in the East. Among Arabjans itappeors to be common. It did not. However, this is correct: however, the facts are correct. It extends itself to the Greeks until after the establishment of Christianity. And it has only assumed its prosent form since the division of the Latin and Greok churches. At which time is correct: at which time. The idea is becoming prevalent. That a Lcltin body could not be corrvpl if buried in their territory. It gradually increased. And formed the subject of many wonderful stories. The fact is that there are still a lot of fossils that are still extant. The dead are rising from their graves. And feeding on the blood of tho young and beautiful. In the West it spreads throughout the world. With some slight variation. All over Hungary, Hungary is correct. Poland is correct. Austria is correct. And Lorraine is correct. Whoro the helies existed? Who the hells existed? That vompyresnightly imbi6ed a certain portion of the blood of their victims. Who became emaciated? They lost their strength. And quickly died of c0nsumptions. While these human blood-suckers fattened—and their veins became distended to such a state of ropletion. As t0 cause the blood to flow from all the passages of their badies. And even for the ucry pores of the skin.\n",
            "In theLond0n Journal. The month of March is correct. 1732, correct: 1732. Is a curiovs. Correct: and so on. Of course, of course. Credible account of a particular case of vampyrifin. Which is stated to have been accurred at Madreyga. In Hungary. It appears that it appears to be correct. Upon an examination of the cornmander-in-chief arid magistrates of the place, it is correct that upon examination of these judges, the judge is correct. They positively and unanimously affirmed that. That is correct: that is correct. About five years before that. A certairi Heyduke. Arnold Paul was named Arnold Paul. Had bcen beclrd to say that. That is correct: that is correct. At Cassovia. Or the descendants of the Turkish Servio are correct. He had been tormented by a vampire. But he had found a way to rid himself of the euj1. By eating some of the earth out of the vampyre's grove. And rubbing himself with his blood. This precaution is correct. However, this is correct: however, the facts are correct. Did not prevent him from bccoming a vampyre himsels. Sorrow: sorrow. About twenty or thirty days after his death and burial. Many persons complain of being tortured by him. And a deposition was made. Four persons had been deprived of life by his attacks. To prevent further mischief. The lnhabitants havjng consulted their Hadagni. Correct: took up the body. And f ound it (as is supposed to be usual in cafes of utopia) fresh. And entjrely free from corruption. Correct and emitting at the rearnouth. riose, you are correct. And ears are correct. Pure and healthy blood. Proof having been thus obtained, proof has been obtained. They resorted to the accustomed remedy. A stake was driven entirely through the bear and body of Arnold Paul. At which he is reported to have cried out dreadfully as is he had been olive. This is done. They cut of his head. He burned his body. And threw his asbes into his grave. The same measures were adopted with the cases of the same persons who had previously dicked from varnpyrism. Lest they should be correct. In their turn, they are correct. Become clgentf upan others who survived them.\n",
            "The universe1 belief js. That a person sucked by a vampyre becomes a vampire himself. Arid sucks in his turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is he.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ocr_data['0'][:3000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTMGMXN8qkRb",
        "outputId": "6738f9b4-cfec-4add-ef57-2a593de4221c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE VAMPYRE;\n",
            "A Tale.\n",
            "By John William Polidori\n",
            "THEsuperstition upon which this taIe iſ founded is very general in the East. Among tho Arabjans itappeors to be common: it did not, however, extend itself to the Greeks until after the establi shment of Christianity; and it has only aſsumed its prosent form since the division af the Latin and Greok churches; at which time, lhe idea becoming prevalent, that a Lcltin body could not corrvpl if buried in their territory, it gradually increosed, and formed lhe subject of many wonderful stories, ſtill extant, of the dead rising from their graves, and feeding uponlhe blood of tho young and beautiful. In the West itspread, with some slight variation, all over Hungary, Poland, Austria, and Lorraine, whoro the helies existed, that vompyresnightly imbi6ed a certain portion of the blood of their victims, who became emaciated, lost their strength, and speedily died of c0nsumptions; whilst these human blood-suckers fattened—and their veins became distended to such a state of ropletion, as t0 cause the blood to flow from all the passages of their badies, and even fr0m the ucry pores of thoir skins.\n",
            "In theLond0n Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to hove accurred at Madreyga, in Hungary. It appears, that upon an examination of the cornmander-in-chief arid magistrates of tbe place, they positively and unanimously affirmed, that, about five years before, a certairi Heyduke, named Arnold Paul, had bcen beclrd to say, that, at Cassovia, ori the fr0ntiers of the Turkish Servio, he had been tormented by a vampyre, but had found a way to rid himself of the euj1, by eating some of the earth out of the vampyre's grove, and rubbing himſelf with his blood. This prccaution, however, did not prevent him from bccoming a vampyre himſels; sor, about twenty or thirty days after his death and burial, many persons complainod of hauing 6een tormented by him, and a deposition was made, that four persons had been deprived os life by his attacks. To prevent further mischief, the lnhabitants havjng consulted their Hadagni, took up the body, and f ound it (aſ is supposed to be usual in cafes of uampyrism) fresh, and entjrely free from corruptjon, and emitting at the rnouth, riose, and ears, pure and fIorid blood. Proof having been thus obtained, they resorted to the accustomed remedy. A stake was driven entirely lhrough the bearl and body of Arnold Paul, at which he is reported to hauecried out cls dreadfully as is he had been olive. This done, they cut ofs his head, burned his body, and threw lhe asbes into his grave. The same measures were adopted with the corses of th ose persons who had previously dicd from varnpyrism, lest they should, in theirturn, become clgentf upan others who survived them.\n",
            "The universa1 belief js, that a person sucked by a vampyre becomes a vampyre himself, arid sucks in his turn.\n",
            "Chlef bai1iff.\n",
            "This monstrous rodomontade is he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corrected_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNSPUzTostFg",
        "outputId": "86d8330c-2474-4e63-fc1f-224590126885"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3686"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New problems : the time computation starts beeing very high : 3 min to apply on 3000 characters. When we will generalize it to the 48 texts, it's going to take hours. Moreover, the outputs are not correct :    \n",
        "\n",
        "- Noisy : In theLond0n Journal, of March, 1732, is a curiovs, and, of course, credible account of a particular case of vampyrifin, which is stated to hove accurred at Madreyga, in Hungary.\n",
        "\n",
        "- Cleaned : In theLond0n Journal. The month of March is correct. 1732, correct: 1732. Is a curiovs. Correct: and so on. Of course, of course. Credible account of a particular case of vampyrifin. Which is stated to have been accurred at Madreyga. In Hungary."
      ],
      "metadata": {
        "id": "TN3c7h57xGXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "!pip install jiwer --quiet\n",
        "from jiwer import wer, cer\n",
        "\n",
        "total_cer = 0\n",
        "total_wer = 0\n",
        "\n",
        "for i in range(48):\n",
        "    ref = clean_data_text[i]\n",
        "    hyp = results[str(i)]\n",
        "    total_cer += cer(ref, hyp)\n",
        "    total_wer += wer(ref, hyp)\n",
        "\n",
        "print(f\"Mean CER: {total_cer / 48:.4f}\")\n",
        "print(f\"Mean WER: {total_wer / 48:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "AMfkb5L1XF1H",
        "outputId": "b564d2a6-98a5-4e0f-d192-44b563510039"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/3.1 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/3.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-051abed6f5cd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_data_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mhyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtotal_cer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_wer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mwer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/measures.py\u001b[0m in \u001b[0;36mcer\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform, return_dict, truth, truth_transform)\u001b[0m\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m     output = process_characters(\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/process.py\u001b[0m in \u001b[0;36mprocess_characters\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;31m# it's the same as word processing, just every word is of length 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     result = process_words(\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/process.py\u001b[0m in \u001b[0;36mprocess_words\u001b[0;34m(reference, hypothesis, reference_transform, hypothesis_transform)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# pre-process reference and hypothesis by applying transforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     ref_transformed = _apply_transform(\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_reference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jiwer/process.py\u001b[0m in \u001b[0;36m_apply_transform\u001b[0;34m(sentence, transform, is_reference)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0mtransformed_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_non_empty_lists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         ):\n\u001b[0;32m--> 369\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0;34m\"After applying the transformation, each reference should be a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0;34m\"non-empty list of strings, with each string being a single word.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: After applying the transformation, each reference should be a non-empty list of strings, with each string being a single word."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideas"
      ],
      "metadata": {
        "id": "H5aPvLoGX0R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possible ideas : get some data from the internet, pass it through an OCR, get the noisy text, and use it to finetune a model ?"
      ],
      "metadata": {
        "id": "WeGMTxI9Xny2"
      }
    }
  ]
}